# M√¥ h√¨nh Ng√¥n ng·ªØ Th·ªã gi√°c

## 1. S·ª≠ d·ª•ng M√¥ h√¨nh Ng√¥n ng·ªØ Th·ªã gi√°c (Vision Language Models)

M√¥ h√¨nh Ng√¥n ng·ªØ Th·ªã gi√°c (VLMs) x·ª≠ l√Ω ƒë·∫ßu v√†o h√¨nh ·∫£nh c√πng v·ªõi vƒÉn b·∫£n ƒë·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• nh∆∞ ch√∫ th√≠ch c·ªßa ·∫£nh, tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng h√¨nh ·∫£nh v√† suy lu·∫≠n ƒëa ph∆∞∆°ng th·ª©c (multimodal).

M·ªôt ki·∫øn tr√∫c VLM ƒëi·ªÉn h√¨nh bao g·ªìm:
1. B·ªô m√£ h√≥a h√¨nh ·∫£nh (*image encoder*) ƒë·ªÉ tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c
2. L·ªõp chi·∫øu (*projection layer*) ƒë·ªÉ cƒÉn ch·ªânh c√°c bi·ªÉu di·ªÖn th·ªã gi√°c v·ªõi vƒÉn b·∫£n
3. M√¥ h√¨nh ng√¥n ng·ªØ ƒë·ªÉ x·ª≠ l√Ω ho·∫∑c t·∫°o vƒÉn b·∫£n. ƒêi·ªÅu n√†y cho ph√©p m√¥ h√¨nh thi·∫øt l·∫≠p c√°c k·∫øt n·ªëi gi·ªØa c√°c y·∫øu t·ªë v·ªÅ th·ªã gi√°c v√† c√°c kh√°i ni·ªám trong ng√¥n ng·ªØ.

T√πy thu·ªôc v√†o t·ª´ng tr∆∞·ªùng h·ª£p m√† c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c VLMs ƒë∆∞·ª£c hu·∫•n luy·ªán theo c√°c t√°c v·ª• kh√°c nhau. C√°c m√¥ h√¨nh c∆° s·ªü (base models) x·ª≠ l√Ω c√°c t√°c v·ª• th·ªã gi√°c-ng√¥n ng·ªØ t·ªïng qu√°t, trong khi c√°c bi·∫øn th·ªÉ t·ªëi ∆∞u h√≥a cho tr√≤ chuy·ªán (chat-optimized variants) h·ªó tr·ª£ c√°c t∆∞∆°ng t√°c h·ªôi tho·∫°i. M·ªôt s·ªë m√¥ h√¨nh bao g·ªìm c√°c th√†nh ph·∫ßn b·ªï sung ƒë·ªÉ l√†m r√µ d·ª± ƒëo√°n d·ª±a tr√™n c√°c b·∫±ng ch·ª©ng th·ªã gi√°c (*visual evidence*) ho·∫∑c chuy√™n v·ªÅ c√°c t√°c v·ª• c·ª• th·ªÉ nh∆∞ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng (*object detection*).

ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt v·ªÅ k·ªπ thu·∫≠t v√† c√°ch s·ª≠ d·ª•ng VLMs, h√£y tham kh·∫£o trang [S·ª≠ d·ª•ng VLM](./vlm_usage.md).

## 2. Tinh ch·ªânh M√¥ h√¨nh Ng√¥n ng·ªØ Th·ªã gi√°c (VLM)

Tinh ch·ªânh VLM l√† vi·ªác ƒëi·ªÅu ch·ªânh m·ªôt m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc (*pre-trained*) ƒë·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• c·ª• th·ªÉ ho·∫∑c ƒë·ªÉ ho·∫°t ƒë·ªông hi·ªáu qu·∫£ tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu c·ª• th·ªÉ. Qu√° tr√¨nh n√†y c√≥ th·ªÉ tu√¢n theo c√°c ph∆∞∆°ng ph√°p nh∆∞ tinh ch·ªânh c√≥ gi√°m s√°t (*supervised fine-tuning*), t·ªëi ∆∞u h√≥a t√πy ch·ªçn (*preference optimization*) ho·∫∑c ph∆∞∆°ng ph√°p k·∫øt h·ª£p (*hybrid approach*) c·∫£ hai, nh∆∞ ƒë√£ gi·ªõi thi·ªáu trong Ch∆∞∆°ng 1 v√† Ch∆∞∆°ng 2.

M·∫∑c d√π c√°c c√¥ng c·ª• v√† k·ªπ thu·∫≠t c·ªët l√µi v·∫´n t∆∞∆°ng t·ª± nh∆∞ c√°c c√¥ng c·ª• v√† k·ªπ thu·∫≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c√°c m√¥ h√¨nh ng√¥n ng·ªØ (LLMs), vi·ªác tinh ch·ªânh VLMs ƒë√≤i h·ªèi ph·∫£i t·∫≠p trung nhi·ªÅu h∆°n v√†o vi·ªác bi·ªÉu di·ªÖn v√† chu·∫©n b·ªã d·ªØ li·ªáu cho h√¨nh ·∫£nh. ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o m√¥ h√¨nh t√≠ch h·ª£p v√† x·ª≠ l√Ω hi·ªáu qu·∫£ c·∫£ d·ªØ li·ªáu th·ªã gi√°c v√† vƒÉn b·∫£n ƒë·ªÉ ƒë·∫°t hi·ªáu su·∫•t t·ªëi ∆∞u. V√¨ m√¥ h√¨nh demo, SmolVLM, l·ªõn h∆°n ƒë√°ng k·ªÉ so v·ªõi m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong b√†i tr∆∞·ªõc, ƒëi·ªÅu c·∫ßn thi·∫øt l√† ph·∫£i kh√°m ph√° c√°c ph∆∞∆°ng ph√°p tinh ch·ªânh hi·ªáu qu·∫£. C√°c k·ªπ thu·∫≠t nh∆∞ l∆∞·ª£ng t·ª≠ h√≥a (*quantization*) v√† Tinh ch·ªânh hi·ªáu qu·∫£ tham s·ªë - PEFT (*Parameter-Efficient Fine-Tuning*) c√≥ th·ªÉ gi√∫p l√†m cho qu√° tr√¨nh n√†y d·ªÖ ti·∫øp c·∫≠n h∆°n v√† ti·∫øt ki·ªám chi ph√≠ h∆°n, cho ph√©p nhi·ªÅu ng∆∞·ªùi d√πng th·ª≠ nghi·ªám v·ªõi m√¥ h√¨nh h∆°n.

ƒê·ªÉ ƒë∆∞·ª£c h∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ tinh ch·ªânh VLMs, h√£y truy c·∫≠p trang [Tinh ch·ªânh VLM](./vlm_finetuning.md).

## B√†i t·∫≠p th·ª±c h√†nh

| Ti√™u ƒë·ªÅ | M√¥ t·∫£ | B√†i t·∫≠p | Link | Colab |
|-------|-------------|----------|------|-------|
| S·ª≠ d·ª•ng VLM | T√¨m hi·ªÉu c√°ch t·∫£i v√† s·ª≠ d·ª•ng VLM ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc cho c√°c t√°c v·ª• kh√°c nhau | üê¢ X·ª≠ l√Ω m·ªôt h√¨nh ·∫£nh<br>üêï X·ª≠ l√Ω nhi·ªÅu h√¨nh ·∫£nh v·ªõi x·ª≠ l√Ω h√†ng lo·∫°t <br>ü¶Å X·ª≠ l√Ω to√†n b·ªô video| [Notebook](./notebooks/vlm_usage_sample.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/5_vision_language_models/notebooks/vlm_usage_sample.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| Tinh ch·ªânh VLM | T√¨m hi·ªÉu c√°ch tinh ch·ªânh VLM ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc cho c√°c t·∫≠p d·ªØ li·ªáu theo t·ª´ng nhi·ªám v·ª• | üê¢ S·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu c∆° b·∫£n ƒë·ªÉ tinh ch·ªânh<br>üêï Th·ª≠ t·∫≠p d·ªØ li·ªáu m·ªõi<br>ü¶Å Th·ª≠ nghi·ªám v·ªõi c√°c ph∆∞∆°ng ph√°p tinh ch·ªânh thay th·∫ø | [Notebook](./notebooks/vlm_sft_sample.ipynb)| <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/5_vision_language_models/notebooks/vlm_sft_sample.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |

## T√†i li·ªáu tham kh·∫£o

- [Hugging Face Learn: Tinh ch·ªânh c√≥ gi√°m s√°t VLMs](https://huggingface.co/learn/cookbook/fine_tuning_vlm_trl)
- [Hugging Face Learn: Tinh ch·ªânh c√≥ gi√°m s√°t SmolVLM](https://huggingface.co/learn/cookbook/fine_tuning_smol_vlm_sft_trl)
- [Hugging Face Learn: Tinh ch·ªânh t·ªëi ∆∞u h√≥a t√πy ch·ªçn SmolVLM](https://huggingface.co/learn/cookbook/fine_tuning_vlm_dpo_smolvlm_instruct)
- [Hugging Face Blog: T·ªëi ∆∞u h√≥a t√πy ch·ªçn cho VLMs](https://huggingface.co/blog/dpo_vlm)
- [Hugging Face Blog: M√¥ h√¨nh Ng√¥n ng·ªØ Th·ªã gi√°c](https://huggingface.co/blog/vlms)
- [Hugging Face Blog: SmolVLM](https://huggingface.co/blog/smolvlm)
- [Hugging Face Model: SmolVLM-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct)
- [CLIP: Learning Transferable Visual Models from Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- [Align Before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/abs/2107.07651)
